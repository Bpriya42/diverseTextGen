# ============================================================================
# Diverse Text Generation - Environment Configuration
# ============================================================================
# Copy this file to .env and customize for your environment.
# All variables are optional - defaults are set in config/settings.py

# ============================================================================
# PATHS
# ============================================================================

# Data directory - where corpus and queries are stored
# RAG_DATA_DIR=./data

# Output directory - where results are saved
# RAG_OUTPUT_DIR=./output

# Cache directory - where embeddings and indices are cached
# RAG_CACHE_DIR=/path/to/cache

# Server logs directory - where vLLM server info is stored
# RAG_SERVER_LOGS_DIR=./server_logs

# Path to log file containing vLLM server URL (host and port)
# RAG_SERVER_LOG_FILE=./server_logs/log.txt

# Path to corpus file
# RAG_CORPUS_PATH=./data/antique/corpus_filtered_50.jsonl

# LangGraph checkpointer path
# RAG_CHECKPOINTER_PATH=./.checkpoints.sqlite

# ============================================================================
# MODEL CONFIGURATION
# ============================================================================

# Default LLM model name
# RAG_DEFAULT_MODEL=Qwen/Qwen3-4B-Instruct-2507

# Use HuggingFace direct inference instead of vLLM server (true/false)
# RAG_USE_HF_DIRECT=false

# Sentence transformer model for embeddings
# RAG_EMBEDDING_MODEL=Snowflake/snowflake-arctic-embed-l

# NLI model for ICAT evaluation
# RAG_NLI_MODEL_NAME=MoritzLaurer/DeBERTa-v3-base-mnli-fever-anli

# ============================================================================
# RETRIEVAL SETTINGS
# ============================================================================

# Number of documents to retrieve per aspect
# RAG_DEFAULT_TOP_K=5

# Batch size for embedding computation
# RAG_EMBEDDING_BATCH_SIZE=512

# ============================================================================
# LLM SETTINGS
# ============================================================================

# Maximum retries for LLM API calls
# RAG_LLM_MAX_RETRIES=10

# Number of parallel workers for LLM
# RAG_LLM_NUM_WORKERS=1

# Planner agent settings
# RAG_PLANNER_TEMPERATURE=0.7
# RAG_PLANNER_MAX_TOKENS=512

# Synthesizer agent settings
# RAG_SYNTHESIZER_TEMPERATURE=0.7
# RAG_SYNTHESIZER_MAX_TOKENS=512

# Verifier agent settings
# RAG_VERIFIER_TEMPERATURE=0.3
# RAG_VERIFIER_MAX_TOKENS=256

# NLI and LLM batch sizes for evaluation
# RAG_NLI_BATCH_SIZE=8
# RAG_LLM_BATCH_SIZE=4

# ============================================================================
# LANGGRAPH SETTINGS (Memory Constraints)
# ============================================================================

# Memory limits for termination (percentage)
# System terminates when RAM or GPU memory exceeds these limits
# RAG_MAX_RAM_PERCENT=90
# RAG_MAX_GPU_PERCENT=90

# ============================================================================
# HUGGINGFACE TOKEN (required for some models)
# ============================================================================
# HF_TOKEN=your_huggingface_token_here

