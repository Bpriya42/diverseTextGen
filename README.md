# Diverse Text Generation - Multi-Agent RAG System

An iterative multi-agent RAG (Retrieval-Augmented Generation) system built with LangGraph for generating diverse, comprehensive, and factually accurate responses.

## Overview

This system implements a 6-agent pipeline that iteratively refines responses through:

1. **Planner (Agent 1)**: Decomposes queries into aspects/perspectives
2. **Retriever (Agent 2)**: Retrieves relevant documents for each aspect
3. **Synthesizer (Agent 3)**: Generates comprehensive answers
4. **Fact Extractor (Agent 4)**: Extracts atomic facts from answers
5. **Verifier (Agent 5)**: Verifies facts against evidence
6. **Coverage Evaluator (Agent 6)**: Evaluates topic coverage

Agents 5 and 6 run in parallel for improved performance (40-50% faster iterations).

**Termination:** The system automatically terminates when:
- Quality criteria are met (no refuted facts, minimal unclear facts, comprehensive coverage), OR
- Memory limits are exceeded (RAM/GPU usage thresholds)

## Project Structure

```
diverseTextGen/
‚îú‚îÄ‚îÄ state.py               # LangGraph state schema
‚îú‚îÄ‚îÄ graph.py               # LangGraph workflow construction
‚îú‚îÄ‚îÄ requirements.txt       # Python dependencies
‚îú‚îÄ‚îÄ env.example            # Environment variable template
‚îÇ
‚îú‚îÄ‚îÄ config/                # Configuration module
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îî‚îÄ‚îÄ settings.py        # All configurable settings
‚îÇ
‚îú‚îÄ‚îÄ agents/                # Agent implementations (logic)
‚îÇ   ‚îú‚îÄ‚îÄ planner.py
‚îÇ   ‚îú‚îÄ‚îÄ retriever.py
‚îÇ   ‚îú‚îÄ‚îÄ synthesizer.py
‚îÇ   ‚îú‚îÄ‚îÄ fact_extractor.py
‚îÇ   ‚îú‚îÄ‚îÄ verifier.py
‚îÇ   ‚îî‚îÄ‚îÄ coverage_evaluator.py
‚îÇ
‚îú‚îÄ‚îÄ nodes/                 # LangGraph node wrappers
‚îÇ   ‚îú‚îÄ‚îÄ planner.py
‚îÇ   ‚îú‚îÄ‚îÄ retriever.py
‚îÇ   ‚îú‚îÄ‚îÄ synthesizer.py
‚îÇ   ‚îú‚îÄ‚îÄ fact_extractor.py
‚îÇ   ‚îú‚îÄ‚îÄ parallel_verification.py
‚îÇ   ‚îî‚îÄ‚îÄ iteration_gate.py
‚îÇ
‚îú‚îÄ‚îÄ llm/                   # LLM clients and prompts
‚îÇ   ‚îú‚îÄ‚îÄ server_llm.py      # vLLM server client
‚îÇ   ‚îú‚îÄ‚îÄ hf_llm.py          # HuggingFace direct inference
‚îÇ   ‚îî‚îÄ‚îÄ prompts/           # Prompt templates (optional)
‚îÇ
‚îú‚îÄ‚îÄ retrieval/             # Dense retrieval system
‚îÇ   ‚îî‚îÄ‚îÄ retriever.py
‚îÇ
‚îú‚îÄ‚îÄ data/                  # Data handling
‚îÇ   ‚îú‚îÄ‚îÄ formatters.py
‚îÇ   ‚îî‚îÄ‚îÄ dataset.py
‚îÇ
‚îú‚îÄ‚îÄ eval/                  # Evaluation modules
‚îÇ   ‚îú‚îÄ‚îÄ icat.py            # ICAT-A evaluation
‚îÇ   ‚îú‚îÄ‚îÄ llm_evaluator.py
‚îÇ   ‚îú‚îÄ‚îÄ retriever.py
‚îÇ   ‚îú‚îÄ‚îÄ experiment_tracker.py
‚îÇ   ‚îî‚îÄ‚îÄ visualizer.py
‚îÇ
‚îú‚îÄ‚îÄ scripts/               # Entry point scripts
‚îÇ   ‚îú‚îÄ‚îÄ run_langgraph.py   # Main RAG runner
‚îÇ   ‚îú‚îÄ‚îÄ run_baseline_experiment.py
‚îÇ   ‚îú‚îÄ‚îÄ run_full_experiment.py
‚îÇ   ‚îú‚îÄ‚îÄ evaluate_icat.py
‚îÇ   ‚îú‚îÄ‚îÄ compare_runs.py
‚îÇ   ‚îî‚îÄ‚îÄ *.sh               # SLURM job scripts
‚îÇ
‚îú‚îÄ‚îÄ artifacts/             # Generated outputs (gitignored)
‚îÇ   ‚îú‚îÄ‚îÄ runs/
‚îÇ   ‚îú‚îÄ‚îÄ outputs/
‚îÇ   ‚îî‚îÄ‚îÄ logs/
‚îÇ
‚îî‚îÄ‚îÄ docs/                  # Documentation
```

## Setup

### 1. Create and activate virtual environment

```bash
# Using conda (recommended)
conda create -n rag python=3.10
conda activate rag

# Or use an existing environment
source /path/to/conda/etc/profile.d/conda.sh
conda activate /path/to/env
```

### 2. Install dependencies

```bash
pip install -r requirements.txt
```

### 3. Configure environment

Copy the example environment file and customize:

```bash
cp env.example .env
# Edit .env with your paths
```

Or set environment variables directly:
```bash
export RAG_DATA_DIR="/path/to/data"
export RAG_CACHE_DIR="/path/to/cache"
export RAG_CORPUS_PATH="/path/to/corpus.jsonl"
export RAG_SERVER_LOGS_DIR="/path/to/server_logs"
```

### 4. Start vLLM server

The system requires a vLLM server running:

```bash
# Start vLLM server
python -m vllm.entrypoints.openai.api_server \
    --model Qwen/Qwen3-4B-Instruct-2507 \
    --host 0.0.0.0 \
    --port 8000

# Create server log file with host and port
echo "localhost" > server_logs/log.txt
echo "8000" >> server_logs/log.txt
```

## Usage

All scripts should be run from the project root directory.

### Single Query

```bash
python scripts/run_langgraph.py \
    --query "What causes headaches and how can they be treated?" \
    --query_id "test_001" \
    --output ./output/test_result.json
```

### Batch Processing

```bash
python scripts/run_langgraph.py \
    --input_file ./data/queries.jsonl \
    -n 10 \
    --output ./output/batch_results
```

### With Custom Memory Limits

```bash
python scripts/run_langgraph.py \
    --query "Explain quantum computing" \
    --query_id "quantum_001" \
    --max_ram_percent 85 \
    --max_gpu_percent 85 \
    --output ./output/result.json
```

### Baseline Experiment (No RAG)

```bash
python scripts/run_baseline_experiment.py \
    --queries_path data/antique/train.jsonl \
    -n 10
```

### ICAT Evaluation

```bash
python scripts/evaluate_icat.py \
    --output_path ./output/batch_results \
    --corpus_path ./data/antique/corpus_filtered_50.jsonl
```

### Compare Runs

```bash
python scripts/compare_runs.py --list  # List all runs
python scripts/compare_runs.py --runs run1 run2 --output comparisons/
```

## Input Format

Queries should be in JSONL format:

```json
{"query_id": "q001", "query_description": "What causes headaches?"}
{"query_id": "q002", "query_description": "How to learn programming?"}
```

## Output Format

Results are saved as JSON with structure:

```json
{
  "query_id": "q001",
  "query": "What causes headaches?",
  "final_answer": "...",
  "total_iterations": 3,
  "termination_reason": "quality_complete",
  "iteration_history": [...],
  "total_runtime_seconds": 45.2,
  "timestamps": {...},
  "memory_config": {...}
}
```

**Termination Reasons:**
- `quality_complete`: All quality metrics met (primary)
- `quality_complete_by_agents`: Both verifier and coverage agents indicate completion
- `memory_exceeded: ...`: RAM or GPU memory limit reached

## Configuration Options

See `env.example` for all available environment variables. Key options:

| Parameter | Environment Variable | Default | Description |
|-----------|---------------------|---------|-------------|
| Data directory | `RAG_DATA_DIR` | `./data` | Data storage path |
| Cache directory | `RAG_CACHE_DIR` | See config | Embedding cache path |
| Server log file | `RAG_SERVER_LOG_FILE` | `./server_logs/log.txt` | vLLM server info |
| Default model | `RAG_DEFAULT_MODEL` | `Qwen/Qwen3-4B-Instruct-2507` | LLM model name |
| Max RAM % | `RAG_MAX_RAM_PERCENT` | `90` | RAM usage termination threshold |
| Max GPU % | `RAG_MAX_GPU_PERCENT` | `90` | GPU memory termination threshold |
| Top-K retrieval | `RAG_DEFAULT_TOP_K` | `5` | Documents per aspect |

## Architecture

```
Query ‚Üí Planner ‚Üí Retriever ‚Üí Synthesizer ‚Üí Fact Extractor
                                        ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                                        ‚Üì               ‚Üì
                                    Verifier    Coverage Evaluator
                                        ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                                ‚Üì
                                        Iteration Gate
                                                ‚Üì
                                Check Quality & Memory Constraints
                                                ‚Üì
                                    [Continue or Terminate]
```

**Iteration Control:**
- **Primary:** Quality-based termination (comprehensive, factual answers)
- **Safety:** Memory-based termination (RAM/GPU thresholds)
- **No fixed iteration limits** - runs until quality criteria are met or memory is exhausted

## Documentation

üìö **Complete documentation is available in the `docs/` directory:**

- **[ARCHITECTURE.md](docs/ARCHITECTURE.md)** - Complete technical documentation
  - 6-agent pipeline architecture
  - LangGraph workflow details
  - State management and termination logic
  - Technical implementation details

- **[USAGE.md](docs/USAGE.md)** - How to run experiments
  - Quick start guide
  - Single query and batch processing
  - Command reference
  - Output structure and monitoring

- **[OBSERVABILITY.md](docs/OBSERVABILITY.md)** - LLM decision tracking
  - Real-time decision logging
  - Quality metrics tracking
  - Plateau detection
  - Log analysis examples

## License

MIT License

## Citation

If you use this code, please cite:

```bibtex
@software{diverse_text_gen,
  title={Diverse Text Generation - Multi-Agent RAG System},
  author={...},
  year={2024},
  url={https://github.com/...}
}
```
